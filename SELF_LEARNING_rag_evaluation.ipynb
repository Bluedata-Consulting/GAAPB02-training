{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "461743ad",
   "metadata": {},
   "source": [
    "# RAG Evaluation Using RAGAS\n",
    "\n",
    "## Introduction to Evaluation\n",
    "Evaluation in the context of Retrieval-Augmented Generation (RAG) systems involves assessing the performance of both the retrieval component (how well relevant documents are fetched from a knowledge base)\n",
    "and the generation component (how accurate, relevant, and coherent the generated responses are). A RAG system combines a retriever (e.g., a vector store like FAISS) with a language model (e.g., AzureChatOpenAI) to provide contextually informed responses, reducing issues like hallucinations (incorrect or fabricated information).\n",
    "\n",
    "## Why Do We Use Evaluation?\n",
    "- Evaluation is critical for the following reasons:\n",
    "- Quality Assurance: Ensures the RAG system delivers accurate, relevant, and trustworthy responses.\n",
    "- System Improvement: Identifies weaknesses in retrieval (e.g., irrelevant documents) or generation (e.g., unfaithful answers), guiding optimizations like better embeddings or prompt engineering.\n",
    "- Performance Monitoring: Quantifies system performance to track improvements or regressions over time.\n",
    "- Stakeholder Confidence: Provides metrics to demonstrate the system's reliability to stakeholders or end-users.\n",
    "\n",
    "### The RAGAS framework (Retrieval Augmented Generation Assessment) is used to evaluate RAG systems. It provides metrics like:\n",
    "- Faithfulness: Measures if the generated answer is factually grounded in the retrieved context.\n",
    "- Answer Relevancy: Assesses if the answer directly addresses the user's query.\n",
    "- Context Precision: Checks if the retrieved context contains relevant information with minimal noise.\n",
    "- Context Recall: Ensures all necessary information is retrieved (requires ground truth).\n",
    "\n",
    "This notebook sets up a RAG system using AzureChatOpenAI, AzureOpenAIEmbeddings, and FAISS, generates a synthetic test dataset, and evaluates the system using RAGAS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b676455",
   "metadata": {},
   "source": [
    "Loads environment variables (e.g., API keys) from a .env file for secure configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3104a7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pymupdf --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f0d1f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import DirectoryLoader, PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os \n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "MODEL_NAME = \"gpt-4.1-mini\"\n",
    "EMBEDDING_MODEL_NAME = \"text-embedding-ada-002\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab08bef",
   "metadata": {},
   "source": [
    "\n",
    "Initializes AzureChatOpenAI for response generation and AzureOpenAIEmbeddings for creating document embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db718d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(azure_deployment=MODEL_NAME)\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(model=EMBEDDING_MODEL_NAME)\n",
    "\n",
    "dir_path = r\"datasets/supply_chain\"\n",
    "index_path = r\"RAG/index/\"\n",
    "collection_name = \"supply_chain\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f667d3",
   "metadata": {},
   "source": [
    "## Document Loading\n",
    "This section loads PDF documents from a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45b7b308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded and split 900 document chunks.\n"
     ]
    }
   ],
   "source": [
    "def load_documents():\n",
    "    \"\"\"\n",
    "    Load PDF documents from the specified directory using PyMuPDFLoader.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of loaded documents.\n",
    "    \n",
    "    Raises:\n",
    "        FileNotFoundError: If the directory does not exist.\n",
    "        Exception: For other loading errors.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(dir_path):\n",
    "        raise FileNotFoundError(f\"Directory not found: {dir_path}\")\n",
    "    try:\n",
    "        loader = DirectoryLoader(dir_path, loader_cls=PyMuPDFLoader)\n",
    "        return loader.load()\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "\n",
    "def split_documents(documents):\n",
    "    \"\"\"\n",
    "    Split documents into smaller chunks using RecursiveCharacterTextSplitter.\n",
    "    \n",
    "    Args:\n",
    "        documents (list): List of documents to split.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of document chunks. Returns empty list if no documents.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not documents:\n",
    "            return []\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=200)\n",
    "        return text_splitter.split_documents(documents)\n",
    "    except Exception as e:\n",
    "        print(f\"Error splitting documents: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "# Load and split documents\n",
    "documents = load_documents()\n",
    "documents = split_documents(documents)\n",
    "print(f\"Loaded and split {len(documents)} document chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a0726e",
   "metadata": {},
   "source": [
    "## Vector Store Creation\n",
    "\n",
    "Now splits documents into chunks, and creates a FAISS vector store for retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e80f2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "def create_vectorstore(documents):\n",
    "    \"\"\"\n",
    "    Create and save a new FAISS vector store from documents.\n",
    "    \n",
    "    Args:\n",
    "        documents (list): List of document objects to convert to vectors.\n",
    "    \n",
    "    Returns:\n",
    "        None: If successful, else Exception.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        os.makedirs(index_path, exist_ok=True)\n",
    "        vectorstore = Chroma.from_documents(documents, embeddings,persist_directory=index_path,\n",
    "                                             collection_name=collection_name)\n",
    "        save_vectorstore(vectorstore)\n",
    "        print(\"Vector store created successfully.\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return e\n",
    "\n",
    "def save_vectorstore(vectorstore):\n",
    "    \"\"\"\n",
    "    Save the FAISS vector store to the specified path.\n",
    "    \n",
    "    Args:\n",
    "        vectorstore (FAISS): The vector store to save.\n",
    "    \n",
    "    Returns:\n",
    "        None: If successful, else Exception.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        vectorstore.save_local(index_path)\n",
    "    except Exception as e:\n",
    "        return e\n",
    "\n",
    "def load_vectorstore():\n",
    "    \"\"\"\n",
    "    Load an existing FAISS vector store.\n",
    "    \n",
    "    Returns:\n",
    "        FAISS: Loaded vector store, else Exception.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return Chroma(embedding_function=embeddings,persist_directory=index_path,\n",
    "                      collection_name=collection_name)\n",
    "    except Exception as e:\n",
    "        return e\n",
    "\n",
    "# Load or create vector store\n",
    "if os.path.exists(index_path) and any(os.listdir(index_path)):\n",
    "    vectorstore = load_vectorstore()\n",
    "    vectorstore_retriever = vectorstore.as_retriever(search_kwargs={'k': 5})\n",
    "    print(\"Vector store loaded successfully.\")\n",
    "else:\n",
    "    create_vectorstore(documents)\n",
    "    vectorstore = load_vectorstore()\n",
    "    vectorstore_retriever = vectorstore.as_retriever(search_kwargs={'k': 5})\n",
    "    print(\"Created and loaded new vector store.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b1c58e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66b579f6",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "\n",
    "- Document Loading: Uses DirectoryLoader with PyMuPDFLoader to load PDFs from the data directory.\n",
    "- Document Splitting: Splits documents into chunks (500 characters, 200 overlap) for efficient retrieval.\n",
    "- Vector Store: Creates a FAISS index from document embeddings or loads an existing one from the index directory.\n",
    "- Retriever: Configures the vector store as a retriever, fetching the top 5 relevant documents for a query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a257d66b",
   "metadata": {},
   "source": [
    "## RAG Chain Setup\n",
    "This section defines a RAG chain that validates queries, retrieves relevant documents, and generates answers using the AzureChatOpenAI model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec1c3a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG Chain Response:\n",
      "- A supply chain is a network of partners who collectively convert a basic commodity (upstream) into a finished product (downstream) that is valued by end-customers.\n",
      "- It involves managing material flows from raw materials (like a bauxite mine) through various stages of conversion to finished goods (like a can of cola).\n",
      "- The supply chain also includes the management of returns at each stage, such as rejected materials or recycling of finished products.\n",
      "- It can be described as a sequence of linked organizations (a chain) or a more complex network where organizations are cross-linked with two-way exchanges.\n",
      "- It encompasses the management processes of planning, sourcing, making, delivering, and returning goods.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def validate_query(query):\n",
    "    \"\"\"\n",
    "    Validates a user's query by ensuring it is not empty and has at least 15 characters.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The input query.\n",
    "    \n",
    "    Returns:\n",
    "        str: The query if valid, or an error message if invalid.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not query:\n",
    "            return \"Query cannot be empty, enter a valid query.\"\n",
    "        elif len(query) < 15:\n",
    "            return \"Query is too short, enter a valid query.\"\n",
    "        else:\n",
    "            return query\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "def create_rag_chain(query, relevant_documents):\n",
    "    \"\"\"\n",
    "    Creates and executes a RAG chain to answer a query using retrieved documents.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The user query.\n",
    "        relevant_documents (list): List of retrieved document chunks.\n",
    "    \n",
    "    Returns:\n",
    "        str: The generated response or an error message.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        prompt_template = \"\"\"\n",
    "        Only based on the provided documents, answer the question in points. Do not mention from which document the answer is derived.\n",
    "        Question: {query}\n",
    "        Documents: {docs}\n",
    "        Note: You are a supply chain assistant. If the query is not related to supply chain or the documents do not provide the necessary information, return \"Invalid Query\".\n",
    "        \"\"\"\n",
    "        prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "        valid_query = validate_query(query)\n",
    "        rag_chain = prompt | llm | StrOutputParser()\n",
    "        return rag_chain.invoke({\"query\": valid_query, \"docs\": relevant_documents})\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "# Test the RAG chain\n",
    "query = \"What is Supply Chain?\"\n",
    "relevant_documents = vectorstore_retriever.invoke(query)\n",
    "response = create_rag_chain(query, relevant_documents)\n",
    "print(\"RAG Chain Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccf2f98",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "\n",
    "- Query Validation: Ensures the query is non-empty and at least 15 characters long.\n",
    "- RAG Chain: Constructs a prompt that instructs the model to answer in bullet points, using only the retrieved documents, and to return \"Invalid Query\" if the query is unrelated to supply chain or unsupported by the documents.\n",
    "- Execution: Combines the prompt, AzureChatOpenAI model, and string output parser to generate a response.\n",
    "- Test: Runs a sample query to verify the RAG chain's functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc2bbe2",
   "metadata": {},
   "source": [
    "## Generating Synthetic Test Data with RAGAS\n",
    "To evaluate the RAG system, we need a test dataset with questions, answers, contexts, and ground truth. RAGAS's TestsetGenerator can create synthetic data from documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e7d2b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ragas rapidfuzz --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e18d305c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying CustomNodeFilter:   0%|          | 0/10 [00:00<?, ?it/s]       Node 4a32b931-dd50-4ffe-87d9-0fd2d864664a does not have a summary. Skipping filtering.\n",
      "Node f2bf59b0-b22b-4e45-897a-2bad546dcef1 does not have a summary. Skipping filtering.\n",
      "Node b8d0952a-dc30-4b38-9c56-ede53db5a860 does not have a summary. Skipping filtering.\n",
      "Node 047ff819-9b7e-45ab-bdae-ae21815123f3 does not have a summary. Skipping filtering.\n",
      "Node 6ea3f6ac-263f-4978-8ea4-9d9248f0f765 does not have a summary. Skipping filtering.\n",
      "Node 34d5122b-53c8-4d1a-b463-fa1d2b7ef9ed does not have a summary. Skipping filtering.\n",
      "Generating personas: 100%|██████████| 3/3 [00:02<00:00,  1.32it/s]                                           \n",
      "Generating Scenarios: 100%|██████████| 3/3 [00:06<00:00,  2.13s/it]\n",
      "Generating Samples: 100%|██████████| 6/6 [00:54<00:00,  9.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 6 test cases.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from ragas.testset import TestsetGenerator\n",
    "\n",
    "# Configure the test set generator\n",
    "testset_generator = TestsetGenerator.from_langchain(\n",
    "    llm=llm,\n",
    "    embedding_model=embeddings\n",
    ")\n",
    "\n",
    "# Randomly sample a subset of documents (e.g., 50 out of 902 chunks)\n",
    "sample_size = 10  # Adjust based on your needs\n",
    "random.seed(42)  # For reproducibility\n",
    "sampled_documents = random.sample(documents, min(sample_size, len(documents)))\n",
    "\n",
    "# Generate test dataset with reduced test_size\n",
    "testset = testset_generator.generate_with_langchain_docs(sampled_documents, 5)\n",
    "\n",
    "# Convert test dataset to evaluation format\n",
    "eval_data = {\n",
    "    \"question\": [],\n",
    "    \"answer\": [],\n",
    "    \"contexts\": [],\n",
    "    \"ground_truth\": []\n",
    "}\n",
    "\n",
    "for testcase in testset.samples:\n",
    "    relevant_docs = vectorstore_retriever.invoke(testcase.eval_sample.user_input)\n",
    "    answer = create_rag_chain(testcase.eval_sample.user_input, relevant_docs)\n",
    "    eval_data[\"question\"].append(testcase.eval_sample.user_input)\n",
    "    eval_data[\"answer\"].append(answer)\n",
    "    eval_data[\"contexts\"].append([doc.page_content for doc in relevant_docs])\n",
    "    eval_data[\"ground_truth\"].append(testcase.eval_sample.reference)\n",
    "\n",
    "print(f\"Generated {len(eval_data['question'])} test cases.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221ee10c",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "\n",
    "- TestsetGenerator: Uses AzureChatOpenAI for generating and critiquing test cases, with AzureOpenAIEmbeddings for document embeddings.\n",
    "- Test Data Generation: Creates test cases with a mix of random samples\n",
    "- Evaluation Dataset: For each test case, retrieves relevant documents, generates an answer using the RAG chain, and collects the question, answer, contexts, and ground truth.\n",
    "- Output: Stores the data in a dictionary format suitable for RAGAS evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5536cdd4",
   "metadata": {},
   "source": [
    "## RAG Evaluation with RAGAS\n",
    "This section evaluates the RAG system using RAGAS metrics: faithfulness, answer relevancy, context precision, and context recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ac8b1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  71%|███████   | 17/24 [02:18<01:51, 15.95s/it]Exception raised in Job[2]: TimeoutError()\n",
      "Exception raised in Job[6]: TimeoutError()\n",
      "Exception raised in Job[10]: TimeoutError()\n",
      "Exception raised in Job[14]: TimeoutError()\n",
      "Evaluating:  75%|███████▌  | 18/24 [03:00<02:21, 23.50s/it]Exception raised in Job[16]: TimeoutError()\n",
      "Evaluating:  92%|█████████▏| 22/24 [03:02<00:18,  9.14s/it]Exception raised in Job[18]: TimeoutError()\n",
      "Evaluating: 100%|██████████| 24/24 [03:05<00:00,  7.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAGAS Evaluation Results:\n",
      "{'faithfulness': 0.8163, 'answer_relevancy': 0.9332, 'context_precision': 1.0000, 'context_recall': 0.9583}\n"
     ]
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall\n",
    "from datasets import Dataset\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "\n",
    "# Convert evaluation data to Hugging Face Dataset\n",
    "eval_dataset = Dataset.from_dict(eval_data)\n",
    "\n",
    "# Wrap AzureChatOpenAI for RAGAS compatibility\n",
    "evaluator_llm = LangchainLLMWrapper(llm)\n",
    "evaluator_embeddings = LangchainEmbeddingsWrapper(embeddings)\n",
    "\n",
    "# Run evaluation\n",
    "results = evaluate(\n",
    "    dataset=eval_dataset,\n",
    "    metrics=[\n",
    "        faithfulness,       # Checks if the answer is grounded in the context\n",
    "        answer_relevancy,   # Checks if the answer addresses the question\n",
    "        context_precision,  # Checks if retrieved context is relevant\n",
    "        context_recall      # Checks if all necessary information is retrieved\n",
    "    ],\n",
    "    llm=evaluator_llm,\n",
    "    embeddings=evaluator_embeddings,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "# Print evaluation results\n",
    "print(\"RAGAS Evaluation Results:\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ed496a",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "\n",
    "- Dataset Conversion: Converts the evaluation data into a Hugging Face Dataset for RAGAS.\n",
    "- LLM Wrapper: Wraps AzureChatOpenAI with LangchainLLMWrapper for compatibility with RAGAS.\n",
    "- Metrics: Evaluates the RAG system on:\n",
    "    - Faithfulness: Ensures answers are factually consistent with the context.\n",
    "    - Answer Relevancy: Measures how well answers address the query.\n",
    "    - Context Precision: Assesses the relevance of retrieved documents.\n",
    "    - Context Recall: Checks if all necessary information is retrieved (uses ground truth).\n",
    "- Results: Outputs scores (0 to 1) for each metric, where higher is better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a78079",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f1f5260",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
